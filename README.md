ðŸš€ SHL Assessment Recommendation System

Intelligent LLM-Enhanced Assessment Recommendation API:

This repository contains my complete SHL assessment recommendation system, built according to all requirements in the assignment:

âœ” Crawling + preprocessing SHL catalog

âœ” Embedding retrieval using SentenceTransformers

âœ” Fine-tuned Cross-Encoder reranking

âœ” Balanced recommendations (Technical + Behavioral)

âœ” Evaluation using Recall@10

âœ” FastAPI backend

âœ” Deployed on HuggingFace Spaces

âœ” Submission CSV generator

âœ” Clean GitHub repository for submission

ðŸ”— Live Services

ðŸ”¹ Backend API (FastAPI, HuggingFace Space):

ðŸ‘‰ https://pushpa12234-shl-recommender-hf.hf.space

ðŸ”¹ HuggingFace Code Repository (Backend + UI):

ðŸ‘‰ https://huggingface.co/spaces/Pushpa12234/SHL_RECOMMENDER_UI/tree/main

ðŸ”¹ GitHub Repository (Submission Repo):

ðŸ‘‰ https://github.com/Puspita0707/shl_recommender_api

ðŸ”¹ HuggingFace UI:( Run this to get the frontend)

ðŸ‘‰ https://huggingface.co/spaces/Pushpa12234/SHL_RECOMMENDER_UI


1. System Architecture:
 
 Query â†’ Embedding Retrieval (MiniLM)
       â†’ Top-50 candidates
       â†’ Cross-Encoder Reranker (fine-tuned)
       â†’ Top-10 assessments
       â†’ JSON API Output

This two-stage pipeline ensures high accuracy, semantic relevance, and balanced coverage across skill domains.

ðŸ“Œ 2. Features:

ðŸ§  Semantic Matching

Uses all-MiniLM-L6-v2 to embed both queries and assessment descriptions.

ðŸ”¥ Reranking

Fine-tuned Cross-Encoder on labelled dataset improves accuracy.

âš– Balanced Results

If query mentions behavioral + technical, both types appear.

ðŸ“Š Full Evaluation

Metrics computed on validation/test set:

Recall@10:    0.224

Precision@10: 0.130

MAP@10:       0.390

NDCG@10:      0.469

ðŸ“Œ 3. Repository Structure

/
â”œâ”€â”€ app.py                         # FastAPI endpoint: /health, /recommend

â”œâ”€â”€ recommender.py                 # Embeddings + reranking logic

â”œâ”€â”€ assessments.csv                # Scraped catalog data

â”œâ”€â”€ reranker-model/                # Fine-tuned CrossEncoder model

â”œâ”€â”€ requirements.txt               # Dependencies

â”œâ”€â”€ start.sh                       # Entrypoint for deployment

â””â”€â”€ README.md                      # Documentation



ðŸ“Œ 3. API DOCUMENTATION

/heath

GET /health

response: 

{ "status": "ok" }

/recommend

POST /recommend

request:

{
  "query": "Looking for a Python developer with strong teamwork"
}

response:

{
  "query": "Looking for a Python developer with strong teamwork",
  "recommendations": [
    {
      "assessment_name": "Computer Science (New)",
      "assessment_url": "https://www.shl.com/products/product-catalog/view/computer-science-new/"
    },
    ...
  ]
}


ðŸ“Œ 5. Data Pipeline

âœ” Catalog Scraping:

The product catalog was scraped and normalized into assessments.csv.

âœ” Embedding Generation:

Used MiniLM encoder with normalized embeddings.

âœ” Reranker Training:

Fine-tuned Cross-Encoder using the provided training set of labeled queries.

âœ” Evaluation:

Metrics computed using official ranking measures required in assignment.

ðŸ“Œ 6. Deployment (HuggingFace SPACES)

The system is deployed using:

FastAPI

Uvicorn

HuggingFace Spaces (Docker)

Everything is packaged so that anyone can run:

uvicorn app:app --host 0.0.0.0 --port 7860


ðŸ“Œ 7. Submission CSV

format_submission.py produces CSV in exact format:

| Query   | Assessment_url |
| ------- | -------------- |
| Query 1 | https://...    |
| Query 1 | https://...    |
| Query 2 | https://...    |

ðŸ“Œ 8. Tech Stack

FastAPI (backend)

Uvicorn (server)

Gradio (UI)

SentenceTransformers

Torch

Pandas

ðŸ“Œ 9. Run Locally:

pip install -r requirements.txt
uvicorn app:app --host 0.0.0.0 --port 7860












